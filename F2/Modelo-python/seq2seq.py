# -*- coding: utf-8 -*-
"""Seq2Seq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u8jJP6X__4moI6h58lKwxcAT63QsJL9r

# Modelo secuencia a secuencia - IA
"""

# Importación de librerías necesarias
import numpy as np
import tensorflow as tf
import pickle
from tensorflow.keras import layers, activations, models, preprocessing
from tensorflow.keras.callbacks import EarlyStopping
import kagglehub
#from google.colab import files

path = kagglehub.dataset_download("kausr25/chatterbotenglish")

# Importación de librerías adicionales
from tensorflow.keras import preprocessing, utils
import os
import yaml

# Configuración del directorio de los datos
dir_path = path
files_list = os.listdir(path + os.sep)  # Lista todos los archivos en el directorio de datos

# Inicialización de listas para preguntas y respuestas
questions = list()
answers = list()

# Lectura y procesamiento de los archivos YAML para extraer conversaciones
for filepath in files_list:
    stream = open(dir_path + os.sep + filepath, 'rb')
    docs = yaml.safe_load(stream)       # Carga el archivo YAML
    conversations = docs['conversations']

    # Extracción de preguntas y respuestas de las conversaciones
    for con in conversations:
        if len(con) > 2:                # Manejo de respuestas con múltiples líneas
            questions.append(con[0])    # La primera línea es la pregunta
            replies = con[1:]           # Las líneas restantes son respuestas
            ans = ''
            for rep in replies:
                ans += ' ' + rep
            answers.append(ans)
        elif len(con) > 1:              # Manejo de respuestas de una sola línea
            questions.append(con[0])
            answers.append(con[1])

# Verificar el tipo de data que tiene
if questions:
    print("Questions tiene datos:", questions)
else:
    print("Questions está vacío")

if answers:
    print("Answers tiene datos:", answers)
else:
    print("Answers está vacío")

# Verificacion del tipo de data y su relacion
import pandas as pd
# Crear un DataFrame que relacione cada pregunta con su respuesta
data = {'Question': questions, 'Answer': answers}
df = pd.DataFrame(data)

# Mostrar el DataFrame
df


# Lista de archivos a integrar
archivos = [
            #'conversaciones.csv',
            'conversaciones.csv',
            'conversations_beginners.csv',
            'usa_sports.csv',
            #'conversaciones_complete.csv'
            ]

# Crear una lista vacía para almacenar los DataFrames
dataframes = []

# Leer cada archivo CSV omitiendo la primera fila y sin cabecera
for archivo in archivos:
    df = pd.read_csv(archivo, header=None, skiprows=1, quotechar='"')  # Omitir cabecera, primera fila y considerar comillas dobles
    dataframes.append(df)  # Agregar el DataFrame a la lista

# Combinar todos los DataFrames en uno solo
df_final = pd.concat(dataframes, ignore_index=True)

# Asignar la nueva cabecera: 'question' y 'answer'
df_final.columns = ['question', 'answer']

# Mostrar las primeras filas del DataFrame final
print(df_final.head())

# Guardar el DataFrame combinado en un nuevo archivo CSV
df_final.to_csv('unificado.csv', index=False)

print("¡Archivos combinados exitosamente!")

# Descargar el archivo combinado
#files.download('unificado.csv')

# Cargar el archivo CSV
df = pd.read_csv('unificado.csv')

# Añadir las preguntas y respuestas a las listas existentes
questions.extend(df['question'].tolist())  # Agregar las preguntas al final de la lista
answers.extend(df['answer'].tolist())      # Agregar las respuestas al final de la lista

# Imprimir nuevamente la lista para verificar que si se hayan agregado mas datos.
data = {'Question': questions, 'Answer': answers}
df = pd.DataFrame(data)

# Mostrar el DataFrame
df

# Agrega etiquetas <START> y <END> a las respuestas
answers_with_tags = list()
for i in range(len(answers)):
    if type(answers[i]) == str:
        answers_with_tags.append(answers[i])
    else:
        questions.pop(i)  # Elimina preguntas sin respuestas válidas

answers = list()
for i in range(len(answers_with_tags)):
    answers.append('<START> ' + answers_with_tags[i] + ' <END>')  # Etiquetas para inicio y fin de secuencia

# Tokenización de preguntas y respuestas
tokenizer = preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(questions + answers)  # Genera un índice de palabras basado en el vocabulario
VOCAB_SIZE = len(tokenizer.word_index) + 1   # El tamaño del vocabulario
print('VOCAB SIZE : {}'.format(VOCAB_SIZE))

# Preprocesamiento adicional
from gensim.models import Word2Vec
import re

# Función auxiliar para tokenizar oraciones
def tokenize(sentences):
    tokens_list = []
    vocabulary = []
    for sentence in sentences:
        sentence = sentence.lower()
        sentence = re.sub('[^a-zA-Z]', ' ', sentence)  # Elimina caracteres no alfabéticos
        tokens = sentence.split()
        vocabulary += tokens
        tokens_list.append(tokens)
    return tokens_list, vocabulary

# Descargar el json
import json
# Guardar los tokenizers
with open('tokenizer_encoder_decoder.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

tokenizer_input = preprocessing.text.Tokenizer()  # Para la entrada

# Recuperamos el array
with open('tokenizer_encoder_decoder.pkl', 'rb') as f:
    tokenizer_input = pickle.load(f)

# Guardar los tokenizers como JSON
with open('tokenizer_encoder_decoder.json', 'w') as f:
    json.dump(tokenizer_input.word_index, f)

# Descargar el archivo combinado
#files.download('tokenizer_encoder_decoder.json')

# Preparación de datos para el encoder
tokenized_questions = tokenizer.texts_to_sequences(questions)  # Convierte texto a secuencias de enteros
maxlen_questions = max([len(x) for x in tokenized_questions])  # Longitud máxima de las preguntas
padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')
encoder_input_data = np.array(padded_questions)
print(encoder_input_data.shape, maxlen_questions)

# Preparación de datos para el decoder (entrada)
tokenized_answers = tokenizer.texts_to_sequences(answers)
maxlen_answers = max([len(x) for x in tokenized_answers])  # Longitud máxima de las respuestas
padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')
decoder_input_data = np.array(padded_answers)
print(decoder_input_data.shape, maxlen_answers)

# Preparación de datos para el decoder (salida)
for i in range(len(tokenized_answers)):
    tokenized_answers[i] = tokenized_answers[i][1:]  # Elimina la etiqueta <START> para las salidas
padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')
onehot_answers = utils.to_categorical(padded_answers, VOCAB_SIZE)  # Convierte a formato one-hot
decoder_output_data = np.array(onehot_answers)
print(decoder_output_data.shape)

# Definición del modelo seq2seq (encoder-decoder)
# Encoder
encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions,))
encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(encoder_inputs)
encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(200, return_state=True)(encoder_embedding)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers,))
decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(decoder_inputs)
decoder_lstm = tf.keras.layers.LSTM(200, return_state=True, return_sequences=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax)
output = decoder_dense(decoder_outputs)

# Compilación y resumen del modelo
model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)
model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')
model.summary()

# Entrenamiento del modelo
model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=50, epochs=150)
model.save('model.h5')  # Guarda el modelo entrenado

# Función para construir modelos de inferencia
def make_inference_models():
    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)

    decoder_state_input_h = tf.keras.layers.Input(shape=(200,))
    decoder_state_input_c = tf.keras.layers.Input(shape=(200,))
    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

    decoder_outputs, state_h, state_c = decoder_lstm(
        decoder_embedding, initial_state=decoder_states_inputs)
    decoder_states = [state_h, state_c]
    decoder_outputs = decoder_dense(decoder_outputs)

    decoder_model = tf.keras.models.Model(
        [decoder_inputs] + decoder_states_inputs,
        [decoder_outputs] + decoder_states)

    return encoder_model, decoder_model

# Función para convertir oraciones de entrada a tokens
def str_to_tokens(sentence: str):
    words = sentence.lower().split()
    tokens_list = list()
    for word in words:
        tokens_list.append(tokenizer.word_index[word])
    return preprocessing.sequence.pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')

# Creación de modelos de inferencia
enc_model, dec_model = make_inference_models()

# Loop de prueba
for _ in range(10):
    states_values = enc_model.predict(str_to_tokens(input('Ingresar pregunta: ')))
    empty_target_seq = np.zeros((1, 1))
    empty_target_seq[0, 0] = tokenizer.word_index['start']
    stop_condition = False
    decoded_translation = ''

    while not stop_condition:
        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)
        sampled_word_index = np.argmax(dec_outputs[0, -1, :])
        sampled_word = None

        for word, index in tokenizer.word_index.items():
            if sampled_word_index == index:
                decoded_translation += ' {}'.format(word)
                sampled_word = word

        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:
            stop_condition = True

        empty_target_seq = np.zeros((1, 1))
        empty_target_seq[0, 0] = sampled_word_index
        states_values = [h, c]

    print(decoded_translation)

# Convertirlo a javascript
# tensorflowjs_converter --input_format keras model.h5 encoder_model/
# tensorflowjs_converter --input_format keras model_lite.h5 encoder_model_lite/